# exllamav2
https://github.com/turboderp/exllamav2/releases/download/v0.0.9/exllamav2-0.0.9+cu121-cp311-cp311-linux_x86_64.whl; platform_system == "Linux"
https://github.com/turboderp/exllamav2/releases/download/v0.0.9/exllamav2-0.0.9+cu121-cp311-cp311-win_amd64.whl; platform_system == "Windows"

# flash-attn
# https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.4/flash_attn-2.3.4+cu122torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl; platform_system == "Linux"
# https://github.com/jllllll/flash-attention/releases/download/v2.3.4/flash_attn-2.3.4+cu121torch2.1cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == "Windows"
